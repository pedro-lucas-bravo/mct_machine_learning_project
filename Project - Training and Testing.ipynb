{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "#ms.use('seaborn-muted')\n",
    "import IPython.display as Ipd\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "def SphericalToCartesian(ele, azi, dist):\n",
    "    phi = np.deg2rad(90-ele)\n",
    "    theta = np.deg2rad(azi)\n",
    "    \n",
    "    x = dist * np.sin(phi) * np.cos(theta)\n",
    "    #x=ρsinφcosθ \n",
    "    y = dist * np.sin(phi) * np.sin(theta)\n",
    "    #y=ρsinφsinθ \n",
    "    z = dist * np.cos(phi)\n",
    "    #z=ρcosφ\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def CartesianToSpherical(x, y, z):\n",
    "    dist = np.sqrt(x*x + y*y + z*z)\n",
    "    theta = np.arctan2(y,x)    \n",
    "    phi = np.arccos(z/dist)\n",
    "    ele = 90 - np.rad2deg(phi)\n",
    "    azi = np.rad2deg(theta)\n",
    "    return np.array([round(ele), round(azi), round(dist), theta])\n",
    "\n",
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm\n",
    "\n",
    "\n",
    "#Get all metadata\n",
    "\n",
    "filenames_meta = os.listdir('data/dcase_data/metadata_dev') #Development\n",
    "\n",
    "full_metadata = pd.DataFrame()\n",
    "#all_metadata\n",
    "max_it = 10\n",
    "c = 0\n",
    "for i in range(len(filenames_meta)):\n",
    "    metadata = pd.read_csv('data/dcase_data/metadata_dev/' + filenames_meta[i])  #Development\n",
    "    coord = np.zeros((len(metadata),3))\n",
    "    sphe = np.zeros((len(metadata),4))\n",
    "    for index, row in metadata.iterrows():\n",
    "        coord[index, :] = SphericalToCartesian(row['ele'], row['azi'], row['dist'])\n",
    "        sphe[index, :] = CartesianToSpherical(coord[index, 0], coord[index, 1], coord[index, 2])\n",
    "    metadata['x'] = coord[:,0]\n",
    "    metadata['y'] = coord[:,1]\n",
    "    metadata['z'] = coord[:,2]\n",
    "    \n",
    "    metadata['ele_r'] = sphe[:,0]\n",
    "    metadata['azi_r'] = sphe[:,1]\n",
    "    metadata['dist_r'] = sphe[:,2]\n",
    "    metadata['theta'] = sphe[:,3]\n",
    "    \n",
    "    full_metadata = full_metadata.append(metadata, ignore_index=True, sort=False)\n",
    "    c +=1\n",
    "    if c == max_it:\n",
    "        break\n",
    "print(len(full_metadata))\n",
    "print(len(full_metadata.loc[full_metadata['sound_event_recording'] == 'phone']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 'split1_ir0_ov1_1' 1/10\n",
      "processing 'split1_ir0_ov1_10' 2/10\n",
      "processing 'split1_ir0_ov1_2' 3/10\n",
      "processing 'split1_ir0_ov1_3' 4/10\n",
      "processing 'split1_ir0_ov1_4' 5/10\n",
      "processing 'split1_ir0_ov1_5' 6/10\n",
      "processing 'split1_ir0_ov1_6' 7/10\n",
      "processing 'split1_ir0_ov1_7' 8/10\n",
      "processing 'split1_ir0_ov1_8' 9/10\n",
      "processing 'split1_ir0_ov1_9' 10/10\n",
      "(258, 65536)\n",
      "Done!\n",
      "(24, 65536)\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lin_interp_2d(data, out_size):\n",
    "    \n",
    "    x_in_size = data.shape[1]\n",
    "    y_in_size = data.shape[0]\n",
    "    x_in = np.arange(0,x_in_size)\n",
    "    y_in = np.arange(0,y_in_size)\n",
    "    interpolator = scipy.interpolate.interp2d(x_in, y_in, data, kind='linear')\n",
    "    x_out = np.arange(0,x_in_size-1,((x_in_size-1)/out_size[1]))\n",
    "    y_out = np.arange(0,y_in_size-1,((y_in_size-1)/out_size[0]))\n",
    "    output = interpolator(x_out, y_out)\n",
    "    output = output[0:out_size[0],0:out_size[1]]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def extract_features_target(signal, location):\n",
    "    stft_amp = np.zeros((128,0))\n",
    "    stft_phase = np.zeros((128,0))\n",
    "    for i in range(4):\n",
    "        stft = librosa.stft(signal[i], n_fft=1024, hop_length=512)\n",
    "        stft_amp = np.append(stft_amp, lin_interp_2d(librosa.amplitude_to_db(np.abs(stft)), (128,64)), axis=1)\n",
    "        stft_phase = np.append(stft_phase, lin_interp_2d(np.angle(stft), (128,64)), axis = 1)\n",
    "\n",
    "        #print(stft_amp.shape)\n",
    "        #print(np.std(stft_amp[0]))\n",
    "\n",
    "\n",
    "    stft_amp = sklearn.preprocessing.scale(stft_amp, axis = 1)\n",
    "    stft_phase = sklearn.preprocessing.scale(stft_phase, axis = 1)\n",
    "\n",
    "    feature = np.append(stft_amp.flatten(), stft_phase.flatten())\n",
    "    return feature, np.array(location)\n",
    "\n",
    "sr = 22050\n",
    "filenames_meta = os.listdir('data/dcase_data/metadata_dev') #Development\n",
    "features = np.zeros((len(full_metadata),65536)) #we compute the melspectrogram which once flattened will produce 22144 values\n",
    "target = np.zeros((len(full_metadata),3)) #we store the 2 target values\n",
    "\n",
    "example = 0\n",
    "for i in range(max_it):\n",
    "    \n",
    "    #Metadata\n",
    "    metadata = pd.read_csv('data/dcase_data/metadata_dev/' + filenames_meta[i])  #Development\n",
    "    filename = os.path.splitext(filenames_meta[i])[0]\n",
    "    \n",
    "    print(\"processing '\" + filename + \"' \" + str(i + 1) + \"/\" + str(max_it))\n",
    "    \n",
    "    #Audio track\n",
    "    signal, dummy = librosa.load('data/dcase_data/foa_dev/' + filename + '.wav', sr, mono=False)    \n",
    "    \n",
    "    for s in range(len(metadata)):\n",
    "        if(metadata['sound_event_recording'][s] == 'phone' ):\n",
    "            #print('processing',filenames[i])\n",
    "            start_time = int(metadata['start_time'][s] * sr)\n",
    "            end_time = int(metadata['end_time'][s] * sr)\n",
    "            #print(str(end_time - start_time))\n",
    "            subsignal = signal[:, start_time:end_time]\n",
    "            #features[example,:], target[example,:] = extract_features_target(subsignal, metadata['x'][s],  metadata['y'][s], metadata['z'][s])\n",
    "            features[example,:], target[example,:] = extract_features_target(subsignal, SphericalToCartesian(metadata['ele'][s],  metadata['azi'][s], metadata['dist'][s]))\n",
    "            \n",
    "            #Ipd.display(Ipd.Audio(subsignal, rate=sr))\n",
    "            example += 1\n",
    "print(features.shape)            \n",
    "features = np.delete(features, np.arange(example,features.shape[0], 1, dtype=int), axis=0)\n",
    "target = np.delete(target, np.arange(example,target.shape[0], 1, dtype=int), axis=0)\n",
    "print('Done!')\n",
    "print(features.shape)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best set of parameters {'classifier__activation': 'logistic', 'classifier__alpha': 0.01, 'classifier__hidden_layer_sizes': (30, 20, 10)}\n",
      "associated best score -1.0462702026179753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#creating pipeline\n",
    "#note that here we do not initialize the parameters we want to tune/change\n",
    "#the parameters that we decide to initialize here will be fixed across the grid search\n",
    "#we also need to keep track of the names (between the quotes)\n",
    "#that we selected for the different components of the pipeline\n",
    "#the names are needed when creating the grid of parameters\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        #('dim_red', LinearDiscriminantAnalysis()),\n",
    "        ('classifier', MLPRegressor(max_iter=500, random_state = 1))\n",
    "        ])\n",
    "\n",
    "#n_components = 10\n",
    "#hidden_layer_sizes=(20,5), max_iter=10000, activation='relu'\n",
    "\n",
    "\n",
    "#creating the repeated stratified k-folds\n",
    "#this is not a must, we can do grid search with a simple k-fold\n",
    "#cross validation by setting cv= to a number in the GridSearchCV constructor\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "\n",
    "#defining the parameters range to explore\n",
    "#the name of the parameters is preceeded by the name of the component\n",
    "#in the pipeline followed by two underscore\n",
    "#if you have trouble in identifying the correct, print all parameters and their\n",
    "#names uwith the following commented line\n",
    "#print(pipe.get_params().keys())\n",
    "grid_param = {\n",
    "    #'dim_red__n_components': [3, 2],\n",
    "    'classifier__hidden_layer_sizes': [(128, 128, 128), (30,20,10), (500, 400, 300, 100, 50)],\n",
    "    'classifier__activation': ['logistic', 'relu'],\n",
    "    'classifier__alpha': [0.01, 0.001],\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=pipe,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='r2', #this can be changes to accuracy, f1_micro, etc. or to another classification metric\n",
    "                     cv=rkf, # if you do not want to do repeated kfold, you can set cv=5 to test just on 5 different splits \n",
    "                     n_jobs=-1) #if equal to -1 will use as many CPU as available\n",
    "\n",
    "gd_sr.fit(features, target) #performing the search\n",
    "\n",
    "print('best set of parameters', gd_sr.best_params_)\n",
    "print('associated best score',gd_sr.best_score_)\n",
    "gd_sr."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
